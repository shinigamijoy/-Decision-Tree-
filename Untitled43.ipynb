{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7rzPokrkFidJ59RQ0P4eh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinigamijoy/-Decision-Tree-/blob/master/Untitled43.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Data Collection\n"
      ],
      "metadata": {
        "id": "PmAkSH0S3KD6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeAHztPl2ec5"
      },
      "outputs": [],
      "source": [
        "# Assuming you have data stored in CSV files\n",
        "customer_appointments = pd.read_csv('customer_appointments.csv')\n",
        "employee_schedules = pd.read_csv('employee_schedules.csv')\n",
        "inventory_data = pd.read_csv('inventory_data.csv')\n",
        "customer_feedback = pd.read_csv('customer_feedback.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Exploratory Data Analysis (EDA)**bold text**"
      ],
      "metadata": {
        "id": "j3twoHDO3lgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic EDA for customer_appointments dataframe and this should be done for all data sources\n",
        "print(customer_appointments.info())\n",
        "print(customer_appointments.describe())\n",
        "\n",
        "# Visualize trends and patterns\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.pairplot(customer_appointments)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WsVvjrQU3pEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Data Integration and Cleaning\n"
      ],
      "metadata": {
        "id": "wG7utVwa3Nj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# merge dataframes based on common keys\n",
        "merged_data = pd.merge(customer_appointments, employee_schedules, on='employee_id', how='inner')\n",
        "merged_data = pd.merge(merged_data, inventory_data, on='product_id', how='left')\n",
        "\n",
        "# Handle missing data\n",
        "merged_data = merged_data.fillna(0)\n",
        "\n",
        "# we may need some feature engineering for the up coming models"
      ],
      "metadata": {
        "id": "a3FLLHJx23Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Customer Behavior Analysis\n"
      ],
      "metadata": {
        "id": "zR49eHKm4iSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this, we will use RFM and service type in the model\n",
        "RFM stands for Recency, Frequency, and Monetary value, each corresponding to some key customer trait. These RFM metrics are important indicators of a customer’s behavior because the frequency and monetary value affect a customer’s lifetime value, and recency affects retention, a measure of engagement, adding the service type will show us more about the customer preference i highly recommend that RFM to be the main matric and the sub matric from every customer will include the service type,the outbut values can be interpreted as the average Recency, Frequency, and Monetary values for each cluster. ServiceType columns indicate the presence of each service type in the cluster."
      ],
      "metadata": {
        "id": "vPpBDxUI5zk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your transactional data (replace 'your_data.csv' with your actual file)\n",
        "# Assume you have columns 'CustomerID', 'Timestamp', 'Amount', and 'ServiceType' at least\n",
        "data = pd.read_csv('your_data.csv')\n",
        "\n",
        "# Convert 'Timestamp' to datetime\n",
        "data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
        "\n",
        "# Calculate Recency, Frequency, Monetary, and include ServiceType as a feature\n",
        "current_date = data['Timestamp'].max() + pd.Timedelta(days=1)  # Add one day to the max date to get a reference point\n",
        "\n",
        "rfm_data = data.groupby('CustomerID').agg({\n",
        "    'Timestamp': lambda x: (current_date - x.max()).days,  # Recency: Days since last purchase\n",
        "    'Amount': 'sum',  # Monetary: Total amount spent (you can use mean, max, etc.)\n",
        "    'Timestamp': 'count',  # Frequency: Number of purchases\n",
        "    'ServiceType': lambda x: x.mode().iloc[0]  # Mode of ServiceType as a representative feature\n",
        "}).reset_index()\n",
        "\n",
        "# Rename columns for clarity\n",
        "rfm_data.columns = ['CustomerID', 'Recency', 'Monetary', 'Frequency', 'ServiceType']\n",
        "\n",
        "# One-hot encode the ServiceType feature\n",
        "rfm_data = pd.get_dummies(rfm_data, columns=['ServiceType'], prefix='ServiceType')\n",
        "\n",
        "# Normalize the data (optional but often beneficial for K-Means)\n",
        "rfm_normalized = (rfm_data[['Recency', 'Frequency', 'Monetary', 'ServiceType_A', 'ServiceType_B', 'ServiceType_C']] - rfm_data[['Recency', 'Frequency', 'Monetary', 'ServiceType_A', 'ServiceType_B', 'ServiceType_C']].mean()) / rfm_data[['Recency', 'Frequency', 'Monetary', 'ServiceType_A', 'ServiceType_B', 'ServiceType_C']].std()\n",
        "\n",
        "# Specify the number of clusters\n",
        "num_clusters = 3\n",
        "\n",
        "# Initialize KMeans model\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "kmeans.fit(rfm_normalized)\n",
        "\n",
        "# Add the cluster labels to your original RFM DataFrame\n",
        "rfm_data['Cluster'] = kmeans.labels_\n",
        "\n",
        "# Display the cluster centers\n",
        "cluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=['Recency', 'Frequency', 'Monetary', 'ServiceType_A', 'ServiceType_B', 'ServiceType_C'])\n",
        "print(\"Cluster Centers:\")\n",
        "print(cluster_centers)\n",
        "\n",
        "# Visualize the clusters in 3D\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.scatter(rfm_data['Recency'], rfm_data['Frequency'], rfm_data['Monetary'], c=rfm_data['Cluster'], cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('Recency')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_zlabel('Monetary')\n",
        "\n",
        "plt.title('RFM Customer Segmentation')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yvJlpkcr4nBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Appointment Scheduling Optimization\n"
      ],
      "metadata": {
        "id": "prYPTbbA9doR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used PuLP to create a binary decision variable for each potential appointment slot and formulates constraints to ensure that each employee and customer has at most one appointment at a given time. The objective is to minimize the total number of scheduled appointments. After solving the optimization problem"
      ],
      "metadata": {
        "id": "vjICoygt-ted"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we want to optimize scheduling based on employee availability\n",
        "# we may use optimization libraries like PuLP or scipy.optimize\n",
        "\n",
        "# Example: Using pandas to find available time slots for each employee\n",
        "employee_availability = employee_schedules[['employee_id', 'availability']]\n",
        "available_time_slots = pd.merge(customer_appointments, employee_availability, on='employee_id', how='left')\n",
        "\n",
        "# Implement scheduling optimization algorithms based on your specific requirements\n",
        "from pulp import LpVariable, lpSum, LpProblem, LpMinimize\n",
        "\n",
        "# Assuming 'available_time_slots' DataFrame has columns 'customer_id', 'employee_id', 'preferred_time', 'availability'\n",
        "# we may need to adapt these based on our actual data\n",
        "\n",
        "# Create a PuLP problem\n",
        "prob = LpProblem(\"Scheduling_Optimization\", LpMinimize)\n",
        "\n",
        "# Decision variable: binary variable indicating whether an appointment is scheduled for a specific slot\n",
        "appointments = LpVariable.dicts(\"Appointment\", ((row['customer_id'], row['employee_id'], row['preferred_time']) for index, row in available_time_slots.iterrows()), 0, 1, LpInteger)\n",
        "\n",
        "# Objective function: minimize the number of scheduled appointments\n",
        "prob += lpSum(appointments)\n",
        "\n",
        "# Constraint: each employee can have at most one appointment at a given time\n",
        "for employee_id, group in available_time_slots.groupby('employee_id'):\n",
        "    for time, slots in group.groupby('preferred_time'):\n",
        "        prob += lpSum(appointments[(row['customer_id'], employee_id, time)] for index, row in slots.iterrows()) <= 1\n",
        "\n",
        "# Constraint: each customer can have at most one appointment at a given time\n",
        "for customer_id, group in available_time_slots.groupby('customer_id'):\n",
        "    for time, slots in group.groupby('preferred_time'):\n",
        "        prob += lpSum(appointments[(customer_id, row['employee_id'], time)] for index, row in slots.iterrows()) <= 1\n",
        "\n",
        "# Solve the problem\n",
        "prob.solve()\n",
        "\n",
        "# Check the status of the solution\n",
        "if LpProblem.status[prob.status] == 'Optimal':\n",
        "    # Extract the scheduled appointments\n",
        "    scheduled_appointments = [(customer_id, employee_id, time) for (customer_id, employee_id, time), var in appointments.items() if var.value() == 1]\n",
        "\n",
        "    # Display the scheduled appointments\n",
        "    print(\"Scheduled Appointments:\")\n",
        "    for customer_id, employee_id, time in scheduled_appointments:\n",
        "        print(f\"Customer {customer_id} with Employee {employee_id} at {time}\")\n",
        "else:\n",
        "    print(\"Optimization problem did not find an optimal solution.\")\n",
        "# or insted of printing it here we can save the outbut in the database for the use in ERB or dashboards"
      ],
      "metadata": {
        "id": "ri80l76D9t0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Customer Feedback Sentiment Analysis\n"
      ],
      "metadata": {
        "id": "xJEzqLMI_7dh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses the sentiment analysis pipeline from the transformers library, which is a convenient way to apply a pre-trained sentiment analysis model. It adds a 'sentiment' column to the customer_feedback dataframe with numerical values (1 for positive, 0 for negative)"
      ],
      "metadata": {
        "id": "e83ixlFA-y1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load pre-trained sentiment analysis model\n",
        "sentiment_analysis = pipeline('sentiment-analysis')\n",
        "\n",
        "# Assuming you have a 'feedback_text' column in the customer_feedback dataframe\n",
        "customer_feedback = pd.read_csv('customer_feedback.csv')\n",
        "\n",
        "# Apply the sentiment analysis model to the feedback_text column\n",
        "customer_feedback['sentiment'] = customer_feedback['feedback_text'].apply(lambda x: sentiment_analysis(x)[0]['label'])\n",
        "\n",
        "# Map sentiment labels to numerical values\n",
        "customer_feedback['sentiment'] = customer_feedback['sentiment'].map({'POSITIVE': 1, 'NEGATIVE': 0})\n",
        "\n",
        "# Display the resulting dataframe with sentiment analysis\n",
        "print(customer_feedback)\n"
      ],
      "metadata": {
        "id": "rjMyDfm2_e4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Recommendation System"
      ],
      "metadata": {
        "id": "unb898gBABKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates the recommendation system using collaborative filtering with the Surprise library. It trains a Singular Value Decomposition (SVD) model, makes predictions on the test set, and provides top recommendations for a specific customer based on their predicted ratings.\n"
      ],
      "metadata": {
        "id": "GpUWSmlSAesM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import Reader, Dataset\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import SVD, accuracy\n",
        "\n",
        "# Assuming you have a 'customer_preferences' DataFrame with columns 'customer_id', 'service_id', 'rating'\n",
        "# 'rating' represents the customer's rating for a particular service (e.g., on a scale from 1 to 5)\n",
        "\n",
        "# Load data into Surprise's Dataset\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(customer_preferences[['customer_id', 'service_id', 'rating']], reader)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the collaborative filtering model (SVD algorithm)\n",
        "model = SVD()\n",
        "model.fit(trainset)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.test(testset)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy.rmse(predictions)\n",
        "\n",
        "# Recommend services for a specific customer\n",
        "def get_top_n_recommendations(customer_id, n=5):\n",
        "    # Get all services not rated by the customer\n",
        "    unrated_services = customer_preferences[~customer_preferences['service_id'].isin(customer_preferences[customer_preferences['customer_id'] == customer_id]['service_id'])]\n",
        "\n",
        "    # Make predictions for unrated services\n",
        "    unrated_services['predicted_rating'] = unrated_services['service_id'].apply(lambda service_id: model.predict(customer_id, service_id).est)\n",
        "\n",
        "    # Sort services based on predicted ratings and get the top n recommendations\n",
        "    top_n_recommendations = unrated_services.sort_values(by='predicted_rating', ascending=False).head(n)\n",
        "\n",
        "    return top_n_recommendations[['service_id', 'predicted_rating']]\n",
        "\n",
        "# Example: Get top 5 recommendations for a specific customer (replace 'your_customer_id' with an actual customer ID)\n",
        "customer_id_to_recommend = 'your_customer_id'\n",
        "top_recommendations = get_top_n_recommendations(customer_id_to_recommend, n=5)\n",
        "\n",
        "# Display the top recommendations\n",
        "print(f\"Top 5 recommendations for customer {customer_id_to_recommend}:\")\n",
        "print(top_recommendations)\n"
      ],
      "metadata": {
        "id": "2iw3gQh3AEqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Data Security and Privacy"
      ],
      "metadata": {
        "id": "JOakVQkSBG_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing data security and privacy measures involves securing sensitive information, encrypting data, and following regulatory requirements. The actual implementation will depend on your infrastructure and compliance needs, i will provide a basic illustration of data encryption in Python using the Fernet symmetric encryption scheme. In a real-world scenario, you would need to adapt and extend these practices based on your specific security and privacy requirements. Additionally, consider consulting with security professionals to ensure your implementation aligns with best practices and compliance standards.."
      ],
      "metadata": {
        "id": "raoTv_zJBzjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cryptography.fernet import Fernet\n",
        "\n",
        "# Step 1: Generate a key for encryption and decryption\n",
        "def generate_key():\n",
        "    return Fernet.generate_key()\n",
        "\n",
        "# Step 2: Save the key securely\n",
        "key = generate_key()\n",
        "with open('encryption_key.key', 'wb') as key_file:\n",
        "    key_file.write(key)\n",
        "\n",
        "# Step 3: Load the key for encryption and decryption\n",
        "def load_key():\n",
        "    return open('encryption_key.key', 'rb').read()\n",
        "\n",
        "# Step 4: Encrypt sensitive information\n",
        "def encrypt_data(data, key):\n",
        "    cipher_suite = Fernet(key)\n",
        "    encrypted_data = cipher_suite.encrypt(data.encode('utf-8'))\n",
        "    return encrypted_data\n",
        "\n",
        "# Step 5: Decrypt sensitive information\n",
        "def decrypt_data(encrypted_data, key):\n",
        "    cipher_suite = Fernet(key)\n",
        "    decrypted_data = cipher_suite.decrypt(encrypted_data).decode('utf-8')\n",
        "    return decrypted_data\n",
        "\n",
        "# Example usage:\n",
        "sensitive_information = \"This is sensitive information\"\n",
        "\n",
        "# Encrypt data\n",
        "encrypted_data = encrypt_data(sensitive_information, key)\n",
        "\n",
        "# Decrypt data\n",
        "decrypted_data = decrypt_data(encrypted_data, key)\n",
        "\n",
        "print(\"Original Data:\", sensitive_information)\n",
        "print(\"Encrypted Data:\", encrypted_data)\n",
        "print(\"Decrypted Data:\", decrypted_data)\n"
      ],
      "metadata": {
        "id": "Dp3XKJdRCOlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9 involves comprehensive Documentation and Reporting, which should be carried out within the script and incorporated into the PowerPoint presentation. The reporting process is versatile and can leverage multiple tools. For instance, we have the option to create a dashboard using Python or utilize specialized tools such as Power BI or Tableau. The choice of tool depends on the nature of the output and the preferences of the stakeholders involved"
      ],
      "metadata": {
        "id": "gwAtDWweCaC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 10: Continuous Improvement\n"
      ],
      "metadata": {
        "id": "Y0LyH-CIChdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can establish a feedback loop and iterate on our models and analyses based on new data and changing business needs.\n",
        "\n",
        "Remember, this is a high-level guide, and each step might require more detailed exploration and adaptation to your specific use case."
      ],
      "metadata": {
        "id": "tDMX_hG4Cm5O"
      }
    }
  ]
}